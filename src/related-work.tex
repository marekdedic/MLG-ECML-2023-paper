\section{Related work}

The publication most relevant to our research is \cite{chen_harp_2018}, in which the HARP approach is proposed. Because we directly extend and modify this method, it will be recalled in some detail in Section \ref{sec:harp}. Other important works concerning graph coarsening are \cite{akyildiz_understanding_2020,cai_graph_2022,chen_graph_2022}, which survey numerous coarsening methods, \cite{huang_scaling_2021}, which presents results concerning scalability of graph coarsening, \cite{catalyurek_multithreaded_2012,herrmann_multilevel_2019}, which establish coarsening as a basis for partitioning, and \cite{loukas_graph_2019}, which shows relationships of graph coarsening to properties of the Laplacian. In view of the fact that the HARP approach is a multilevel approach, we paid attention also to the multilevel graph coarsening methods proposed in \cite{bethune_hierarchical_2020,liu_hierarchical_2021,xie_graph_2020,zhang_harp_2021}, among them \cite{zhang_harp_2021} also being inspired by HARP.

In a broader context, our research is related to the more general topic of graph reduction, which apart from graph coarsening includes also graph sparsification and condensation. A general framework covering both coarsening and sparsification has been proposed in \cite{bravo_hermsdorff_unifying_2019}. Graph condensation is a more recent approach \cite{jin_condensing_2022,jin_graph_2022}, inspired by the gradient matching scheme for the construction of training datasets, called dataset condensation \cite{zhao_dataset_2021}: To a given original graph, it attempts to construct a much smaller synthetic graph such that the gradients of the parameters of a considered graph neural network with respect to both graphs match. Also of note is the recent work \cite{kammer_space-efficient_2022}, presenting an alternative coarsening approach for planar graphs and \cite{liu_comprehensive_2022}, which sparsifies not only the graph topology, but simultaneously also the features of its nodes and weights of graph neural network used for its embedding. Elaboration of graph coarsening methods in machine learning can build on several decades of their successful application, such as pairwise aggregation, independent sets, or algebraic distance, in numerical linear algebra \cite{chen_graph_2022}, including in particular multilevel graph coarsening \cite{osei-kuffuor_matrix_2015,ubaru_sampling_2019}.

More recently, a connection of graph coarsening with another more general topic has been addressed, namely with pooling in graph neural networks. In a framework presented in \cite{grattarola_understanding_2022}, pooling is viewed as a composition of three subsequent transformations: selection, in which the nodes of the input graph are mapped to the nodes of the pooled one, reduction, in which the node attributes of the input graph are aggregated into the node attributes of the pooled one, and connection, in which the edges and possibly edge attributes of the input graph are mapped to the edges and possibly edge attributes of the pooled one. This sequence of transformations is perfectly relevant also to coarsening methods, as was in \cite{grattarola_understanding_2022} demonstrated for the methods NMF \cite{bacciu_non-negative_2019} and top-K \cite{cangea_towards_2018,gao_graph_2019}.
