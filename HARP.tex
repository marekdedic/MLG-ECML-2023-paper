\section{HARP}\label{sec:harp}

\subsection{Method overview}

HARP is a method for improving the performance of graph-based learning algorithms such as DeepWalk \cite{perozzi_deepwalk_2014}, LINE \cite{tang_line_2015}, PTE \cite{tang_pte_2015} or node2vec \cite{grover_node2vec_2016}. The method is a combination of dataset augmentation and pre-training based on the general principle that graph-based models train more efficiently on smaller graphs and can thus be pre-trained on a coarsened representation of the graph task at hand. In an overview, the method consists of the following steps:
\begin{enumerate}
  \item \textbf{Dataset augmentation}. The graph is consecutively reduced in size by the application of several graph coarsening schemas. In each step, the coarsened can be viewed as a representation of ever more global structure in the task.
\end{enumerate}
After all the coarsened graphs are pre-computed, the method itself can be executed by repeating the following steps on the graphs from the coarsest to the finest:
\begin{enumerate}[start=2]
  \item \textbf{Training on an intermediary graph}. The model is trained on one of the pre-computed intermediary graphs.
  \item \textbf{Embedding prolongation}. The embedding generated by the model is prolonged from a coarser graph to a finer one (with the original graph being the finest).
\end{enumerate}

The first step is independent of the rest of the computation and can be done ahead of time. The last two steps can be seen as a form of pre-training for the model that is to be learnt on the original task.

\subsection{Graph coarsening}\label{sec:graph-coarsening}
\sloppy Consider an undirected graph \( G \) with nodes \( V \left( G \right) \) and edges \( E \left( G \right) \). The aim of the graph coarsening part of the algorithm is to generate a sequence of graphs \( G_0, G_1, G_2, \dots, G_L \) where \( G_0 = G \) and \( L \in \mathfield{N} \) is a hyperparameter of the method. In this sequence, each graph \( G_i \) is generated from the graph \( G_{i - 1} \) by coarsening it -- lowering the number of nodes and edges while preserving the general structure of the graph. Following \cite{chen_harp_2018}, let \( \psi_i \) denote the mapping between the graphs such that \( G_i = \psi_i \left( G_{i - 1} \right) \).

In Section \ref{sec:harp-as-pihom}, a general framework for graph coarsening is presented. The authors of \cite{chen_harp_2018} instead introduce two particular coarsening relations \( \psi_i \) -- \textbf{edge collapsing} and \textbf{star collapsing}. Edge collapsing is a very simple method -- out of all the edges \( E \left( G_{i - 1} \right) \), a subset \( E' \) is selected such that no two edges from \( E' \) are incident on the same node. Then, for each edge \( \left( u, v \right) \in E' \), \( u \) and \( v \) are merged into a single node \( w \), with all edges incident on \( u \) or \( v \) being replaced with edges incident on \( w \).

The edge collapsing algorithm is a good general way of lowering the number of nodes in a graph, however, some structures are not easily collapsed by it. An example of such a structure is a \enquote{star} -- a single node connected to many other nodes. To coarsen graphs with such structures effectively, the star collapsing algorithm is proposed. For each such \textit{hub} node \( u \), its unconnected neighbouring nodes are taken and merged pairwise. Again, all edges incident on such nodes are replaced with edges incident on the corresponding newly created nodes. These two approaches are combined in HARP, with each coarsening step being a star collapsing step followed by an edge collapsing step.

\subsection{Embedding prolongation}
In each coarsening step, an embedding of the graph \( G_i \) is trained by one of the embedding algorithms. To continue training with a finer graph, this embedding \( \Phi_i: V \left( G_i \right) \to \mathfield{R}^d \) needs to be \textit{prolonged} to create the finer embedding \( \Phi_{i - 1}: V \left( G_{i - 1} \right) \to \mathfield{R}^d \). To achieve this, the representation of a node in the graph \( G_i \) is copied for each of the nodes in \( G_{i - 1} \) it was created from (by the graph collapsing algorithm). That is,
\[ \Phi_{i - 1} \left( u \right) = \Phi_i \left( \psi_i \left( u \right) \right)\text{.} \]
This is then taken as the starting point for the next training phase.
